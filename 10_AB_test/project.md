# Описание проекта

Вы работаете в стартапе, который продаёт продукты питания. Нужно разобраться, как ведут себя пользователи вашего мобильного приложения.

Изучите воронку продаж. Узнайте, как пользователи доходят до покупки. Сколько пользователей доходит до покупки, а сколько — «застревает» на предыдущих шагах? На каких именно?

После этого исследуйте результаты A/A/B-эксперимента. Дизайнеры захотели поменять шрифты во всём приложении, а менеджеры испугались, что пользователям будет непривычно. Договорились принять решение по результатам A/A/B-теста. Пользователей разбили на 3 группы: 2 контрольные со старыми шрифтами и одну экспериментальную — с новыми. Выясните, какой шрифт лучше.

Создание двух групп A вместо одной имеет определённые преимущества. Если две контрольные группы окажутся равны, вы можете быть уверены в точности проведенного тестирования. Если же между значениями A и A будут существенные различия, это поможет обнаружить факторы, которые привели к искажению результатов. Сравнение контрольных групп также помогает понять, сколько времени и данных потребуется для дальнейших тестов.

В случае общей аналитики и A/A/B-эксперимента работайте с одними и теми же данными. В реальных проектах всегда идут эксперименты. Аналитики исследуют качество работы приложения по общим данным, не учитывая принадлежность пользователей к экспериментам.

**Описание данных**

Каждая запись в логе — это действие пользователя, или событие.

- `EventName` — название события;
- `DeviceIDHash` — уникальный идентификатор пользователя;
- `EventTimestamp` — время события;
- `ExpId` — номер эксперимента: 246 и 247 — контрольные группы, а 248 — экспериментальная.

### 1. Откройте файл с данными и изучите общую информацию
```
# импортируем библиотеку обработки и анализа структурированных данных
import pandas as pd
pd.set_option('display.max_colwidth', None)

# импортируем библиотеку для визуализации данных двумерной и трёхмерной графикой
import matplotlib.pyplot as plt

# импортируем библиотеку библиотеку для визуализации данных
# https://habr.com/ru/company/otus/blog/588190/?ysclid=la5ka4qvdj473949173
import plotly.graph_objects as go
import plotly.express as px

# импортируем библиотеку математических вычислений
import numpy as np

# импортируем модуль для работы с датой и временем
import datetime as dt

# импортируем статистические функции
import scipy.stats as stats

# импортируем библиотеку для создания статистических графиков
import seaborn as sns

# импортируем библиотеку с математическими функциями
import math as mth
```
```
# данные из файла logs_exp сохраним в переменную df, установим разделитель \
df = pd.read_csv('/datasets/logs_exp.csv', sep='\t')
```
```
# посмотрим датафрейм
df
```
|        |               EventName |        DeviceIDHash | EventTimestamp | ExpId |
|-------:|------------------------:|--------------------:|---------------:|------:|
|    0   | MainScreenAppear        | 4575588528974610257 | 1564029816     | 246   |
|    1   | MainScreenAppear        | 7416695313311560658 | 1564053102     | 246   |
|    2   | PaymentScreenSuccessful | 3518123091307005509 | 1564054127     | 248   |
|    3   | CartScreenAppear        | 3518123091307005509 | 1564054127     | 248   |
|    4   | PaymentScreenSuccessful | 6217807653094995999 | 1564055322     | 248   |
|   ...  | ...                     | ...                 | ...            | ...   |
| 244121 | MainScreenAppear        | 4599628364049201812 | 1565212345     | 247   |
| 244122 | MainScreenAppear        | 5849806612437486590 | 1565212439     | 246   |
| 244123 | MainScreenAppear        | 5746969938801999050 | 1565212483     | 246   |
| 244124 | MainScreenAppear        | 5746969938801999050 | 1565212498     | 246   |
| 244125 | OffersScreenAppear      | 5746969938801999050 | 1565212517     | 246   |

`244126 rows × 4 columns`
```
df.info()
```
```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 244126 entries, 0 to 244125
Data columns (total 4 columns):
 #   Column          Non-Null Count   Dtype 
---  ------          --------------   ----- 
 0   EventName       244126 non-null  object
 1   DeviceIDHash    244126 non-null  int64 
 2   EventTimestamp  244126 non-null  int64 
 3   ExpId           244126 non-null  int64 
dtypes: int64(3), object(1)
memory usage: 7.5+ MB
```
Получили почти 250 тыс строк. Требуется переименование колонок, изменене типов данных.

### 2. Подготовьте данные

**2.1  Замените названия столбцов на удобные для вас**
```
df = df.rename(columns = {'EventName': 'event', 'DeviceIDHash': 'user_id', 'EventTimestamp': 'event_time', 'ExpId': 'group'})
```
**2.2  Проверьте пропуски и типы данных. Откорректируйте, если нужно**
```
# посмотрим пропуски
df.isna().sum()
```
```
event         0
user_id       0
event_time    0
group         0
dtype: int64
```
Пропуски отсутствуют.
```
# подсчёт явных дубликатов
df.duplicated().sum()
```
`413`   

Присутствуют задублированные строки. Удалим их
```
# удаление явных дубликатов
df = df.drop_duplicates()
```
**2.3  Добавьте столбец даты и времени, а также отдельный столбец дат**
```
# преобразуем UNIX epoch в datetime64
df['datetime'] = pd.to_datetime(
    df['event_time'], unit='s'
)

# создадим столбец с датой
df['date'] = pd.to_datetime(
    df['datetime']) \
        .dt.date \
        .astype('datetime64')

df.head(1)
```
|   |            event |             user_id | event_time | group |            datetime |       date |
|--:|-----------------:|--------------------:|-----------:|------:|--------------------:|-----------:|
| 0 | MainScreenAppear | 4575588528974610257 | 1564029816 | 246   | 2019-07-25 04:43:36 | 2019-07-25 |

### 3.  Изучите и проверьте данные

**3.1  Сколько всего событий в логе?**
```
# посмотрим виды событий и их количество
df['event'].value_counts()
```
```
MainScreenAppear           119101
OffersScreenAppear          46808
CartScreenAppear            42668
PaymentScreenSuccessful     34118
Tutorial                     1018
Name: event, dtype: int64
```
У нас пять типов событий:
- `MainScreenAppear` - пользователь попал на главный экран
- `OffersScreenAppear` - экран оформления заказа
- `CartScreenAppear`- экран корзины
- `PaymentScreenSuccessful` - уведомление об успешной оплате
- `Tutorial` - помощь / руководство
```
events = df['event'].count()
print('Всего событий в логе =', events)
```
```
Всего событий в логе = 243713
```
**3.2  Сколько всего пользователей в логе?**
```
users = len(df['user_id'].unique())
print('Количество пользователей =', users)
```
```
Количество пользователей = 7551
```
**3.3  Сколько в среднем событий приходится на пользователя?**
```
print('Среднее количество событий на пользователя =', int(events/users))
```
```
Среднее количество событий на пользователя = 32
```
```
# методом describe, посмотрим как распределено количество событий у пользователей
number_of_events = pd.DataFrame(df.groupby('user_id')['event'].count())
display(number_of_events.describe())
print()
print('Контрольная группа 1')
print(df.query('group == 246').groupby('user_id')['event'].count().describe())
print()
print('Контрольная группа 2')
print(df.query('group == 247').groupby('user_id')['event'].count().describe())
print()
print('Экспериментальная группа')
print(df.query('group == 248').groupby('user_id')['event'].count().describe())
```
|       |       event |
|------:|------------:|
| count | 7551.000000 |
|  mean | 32.275593   |
|  std  | 65.154219   |
|  min  | 1.000000    |
|  25%  | 9.000000    |
|  50%  | 20.000000   |
|  75%  | 37.000000   |
|  max  | 2307.000000 |

| Контрольная группа 1 |             |   | Контрольная группа 2 |             |   | Экспериментальная группа |             |
|----------------------|-------------|---|----------------------|-------------|---|--------------------------|-------------|
| count                | 2489.000000 |   | count                | 2520.000000 |   | count                    | 2542.000000 |
| mean                 | 32.214142   |   | mean                 | 30.932540   |   | mean                     | 33.667191   |
| std                  | 65.085668   |   | std                  | 56.304849   |   | std                      | 72.931171   |
| min                  | 1.000000    |   | min                  | 1.000000    |   | min                      | 1.000000    |
| 25%                  | 9.000000    |   | 25%                  | 9.000000    |   | 25%                      | 9.000000    |
| 50%                  | 19.000000   |   | 50%                  | 19.500000   |   | 50%                      | 20.000000   |
| 75%                  | 37.000000   |   | 75%                  | 37.000000   |   | 75%                      | 38.000000   |
| max                  | 1998.000000 |   | max                  | 1768.000000 |   | max                      | 2307.000000 |

Медианное количество событий на пользователя равно 20. Максимальное значение похоже на выброс. В каждую группу попали рекордсмены по количеству событий.
Посмотрим последние перцентили.
```
# Посчитаем с 95 по 99 перцентили 
print(np.percentile(number_of_events['event'], [95, 96, 97, 98, 99])) 
```
```
[ 89.  100.  115.5 136.  200.5]
```
```
print('Пользователей, совершивших более 200 действий =', len(number_of_events.query('event > 200')))
```
```
Пользователей, совершивших более 200 действий = 76
```
Построим гистрограмму распредления числа событий на одного пользователя
```
plt.figure(figsize=(15,5))
plt.hist((df.groupby('user_id')['event'].count()), bins=50, ec="white", range=(1, 200))
plt.grid()
plt.xlabel('События')
plt.ylabel('Пользователи')
plt.title('Число событий на пользователя')
plt.show()
```
![изображение](https://user-images.githubusercontent.com/104757775/203414723-fff2206a-3a4f-4528-949a-065d67b30c7a.png)

Полученный график имеет длинный хвост справа. Визуально различимо - до 100 событий. Далее количество пользователй очень мало.
```
# получим ID самого активного пользователя
number_of_events.sort_values('event').tail(1)
```
|       user_id       | event |
|:-------------------:|:-----:|
| 6304868067479728361 | 2307  |
```
# создадим отдельную переменную для активного пользователя
active_user = df.query('user_id == 6304868067479728361')
```
```
# добавим колонку с номером дня
active_user['day'] = active_user['date'].dt.day
```
```
active_user.groupby('day').agg({'event': 'count'}).reset_index()
```
|   | day | event |
|--:|----:|------:|
| 0 | 1   | 68    |
| 1 | 2   | 2190  |
| 2 | 3   | 18    |
| 3 | 4   | 8     |
| 4 | 5   | 5     |
| 5 | 6   | 4     |
| 6 | 7   | 14    |
```
active_user.head(1)
```
|       |            event |             user_id | event_time | group |            datetime |       date | day |
|------:|-----------------:|--------------------:|-----------:|------:|--------------------:|-----------:|----:|
| 32936 | MainScreenAppear | 6304868067479728361 | 1564682485 | 248   | 2019-08-01 18:01:25 | 2019-08-01 | 1   |

События распределены неравномерно. Большая часть приходится на 2-ой день.
```
active_user.query('day == 2').event.value_counts()
```
```
CartScreenAppear           1070
PaymentScreenSuccessful    1063
OffersScreenAppear           38
MainScreenAppear             19
Name: event, dtype: int64
```
Главный экран отобразился 19 раз.   
Экран выбора товара - 38 событий.   
Переход в корзину - 1070 событий.   
Подтверждение оплаты - 1063 событий.

Если это не технический сбой, тогда мы получили оптового покупателя, который, вероятно, выбирал товар через веб интерфейс, а в момент перехода в корзину автоматически открывалось мобильное приложение. Почти все заказы из корзины были оплачены.

Чтобы исключить технический сбой необходимо проверить суммы оплаты и факт получения товара.

**3.4  Данными за какой период вы располагаете? Найдите максимальную и минимальную дату. Постройте гистограмму по дате и времени. Можно ли быть уверенным, что у вас одинаково полные данные за весь период? Технически в логи новых дней по некоторым пользователям могут «доезжать» события из прошлого — это может «перекашивать данные». Определите, с какого момента данные полные и отбросьте более старые. Данными за какой период времени вы располагаете на самом деле?*
**
```
print('Период исследования -', df['date'].max() - df['date'].min())
print('Первая дата лога -', df['date'].min())
print('Последняя дата лога -', df['date'].max())
```
```
Период исследования - 13 days 00:00:00
Первая дата лога - 2019-07-25 00:00:00
Последняя дата лога - 2019-08-07 00:00:00
```
Мы видим, что файл с логами содержит информацию о событиях за 14 деней.
Посмотрим, как распределены события про датам.
```
# построим столбчатую гистограмму 
df.date.hist(bins=14, ec="white", grid=False, figsize=(15,5)) \
    .set_title('Распределение числа событий по датам');
```
![изображение](https://user-images.githubusercontent.com/104757775/203416089-132ff16c-b61b-498d-943f-2b11d26ffca8.png)

Почти все данные распределены в последней неделе исследования.   
Вероятно, из-за технической особенности в логи новых дней загружаются события из прошлого.
```
print('Количество событий в 1-ю неделю =', df.query('date < "2019-08-01"').shape [0])
print('Доля событий 1-ой недели =',  "{:.1%}".format(df.query('date < "2019-08-01"').shape [0] / events))
```
```
Количество событий в 1-ю неделю = 2826
Доля событий 1-ой недели = 1.2%
```
Посмотрим, с какого момента логи событий начали активно сохраняться. Для этого построим почасовую гистограмму за 31 июля.
```
df.query('"2019-08-01" > date > "2019-07-30"') \
    .pivot_table(index='datetime', values='event', aggfunc='count') \
    .reset_index() \
    .datetime.hist(bins=24, ec="white", grid=False, figsize=(15,5)) \
    .set_title('Распределение событий 31 июля по часам');
```
![изображение](https://user-images.githubusercontent.com/104757775/203416389-864562dc-962d-4c15-8836-3d7049eb7763.png)

Активная запись логов начинается после 21-00. Отбросим данные до этого времени.
```
df.head(1)
```
|   |            event |             user_id | event_time | group |            datetime |       date |
|--:|-----------------:|--------------------:|-----------:|------:|--------------------:|-----------:|
| 0 | MainScreenAppear | 4575588528974610257 | 1564029816 | 246   | 2019-07-25 04:43:36 | 2019-07-25 |
```
# отбросим ненужный период
data = df.query('event_time > 1564596000')

# проверим кол-во удалённых данных
events - data.shape [0]
```
```
1762
```
*Не смог применить метод query к колонке `datetime`, поллучаю ошибку `not supported between instances of 'type' and 'str'`. Через Timestamp To Date Converter получил, что для даты и времени 2019-07-31 21:00:00 соответствует 1564596000 timestamp.*
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
```
